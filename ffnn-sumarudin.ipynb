{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"maze_data_list_alot = [\n    \"\"\"\n    -10;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-10;\n    -1;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -10;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -10;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-10;\n    -10;-1;-1;-1;\n    -10;-1;-1;-1;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    \"\"\",\n    \"\"\"\n    -1;-10;-10;-10;\n    -1;-1;-10;-10;\n    -1;-1;-1;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -1;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -1;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -10;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -10;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    \"\"\",\n    \"\"\"\n    -1;-1;-10;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-1;\n    -10;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -1;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -1;-1;-1;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    \"\"\",\n#     \"\"\"\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-10;\n#     -1;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -1;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -1;-10;-1;-1;\n#     -1;-10;-10;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-1;\n#     -10;-1;-1;-1;\n#     -10;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-10;\n#     \"\"\",\n#     \"\"\"\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-1;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-1;\n#     -10;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-1;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-1;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     \"\"\",\n#     \"\"\"\n#     -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -1;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n#     \"\"\",\n# \"\"\"\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -10;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-1;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# \"\"\",\n# \"\"\"\n# -1;-1;-10;-10;\n# -1;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -1;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-1;-10;-1;\n# -1;-1;-1;-10;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# \"\"\",\n# \"\"\"\n# -10;-1;-10;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-10;\n# -1;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -10;-1;-1;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-1;-1;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# \"\"\",\n# \"\"\"\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-1;\n# -10;-1;-1;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -1;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -10;-10;-1;-1;\n# \"\"\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:12.770357Z","iopub.execute_input":"2024-07-30T06:55:12.770704Z","iopub.status.idle":"2024-07-30T06:55:12.792804Z","shell.execute_reply.started":"2024-07-30T06:55:12.770675Z","shell.execute_reply":"2024-07-30T06:55:12.791940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom PIL import Image, ImageDraw\n\nclass CustomMazeEnvCNNIMGML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10):\n        super(CustomMazeEnvCNNIMGML, self).__init__()\n        \n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        \n        # Define image size for DQN\n        self.image_size = 84\n        self.observation_space = spaces.Box(low=0, high=255, shape=(self.image_size, self.image_size, 3), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n        \n        self.state = (0, 0)\n        self.start = (0, 0)\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n\n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n        \n    def initialize_maze(self, input_string):\n        maze_size = self.maze_size\n        rows = input_string.strip().split('\\n')\n        top_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        right_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        bottom_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        left_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n        for i in range(maze_size):\n            for j in range(maze_size):\n                state_index = i * maze_size + j\n                actions = parsed_data[state_index]\n                bottom_wall[i, j] = 1 if actions[0] == -10 else 0\n                right_wall[i, j] = 1 if actions[1] == -10 else 0\n                left_wall[i, j] = 1 if actions[2] == -10 else 0\n                top_wall[i, j] = 1 if actions[3] == -10 else 0\n\n        return np.stack([top_wall, right_wall, bottom_wall, left_wall], axis=0)\n        \n    def _get_obs(self):\n        return self._render_maze()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.visit_counts[self.state] += 1\n        \n        # Change to the next maze\n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        # Check for walls or boundaries\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[0, x, y] == 1):  # Top wall or top edge\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[1, x, y] == 1):  # Right wall or right edge\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[2, x, y] == 1):  # Bottom wall or bottom edge\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[3, x, y] == 1):  # Left wall or left edge\n            wall_hit = True\n        \n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n        \n        return self._get_obs(), self.reward, self.done, False, {\"is_success\": self.is_success}\n\n    \n    def _render_maze(self):\n        img = Image.new('RGB', (self.image_size, self.image_size), color=(255, 255, 255))\n        draw = ImageDraw.Draw(img)\n\n        cell_size = min(self.image_size // self.maze_size, self.image_size // self.maze_size)\n        maze_pixel_size = cell_size * self.maze_size\n        offset_x = (self.image_size - maze_pixel_size) // 2\n        offset_y = (self.image_size - maze_pixel_size) // 2\n\n        # Draw the maze walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                x, y = offset_x + j * cell_size, offset_y + i * cell_size\n                walls = [(0, [(x, y), (x + cell_size, y)]),             # Top\n                        (1, [(x + cell_size, y), (x + cell_size, y + cell_size)]),  # Right\n                        (2, [(x, y + cell_size), (x + cell_size, y + cell_size)]),  # Bottom\n                        (3, [(x, y), (x, y + cell_size)])]            # Left\n                for idx, line in walls:\n                    if self.maze[idx, i, j]:\n                        draw.line(line, fill=(0, 0, 0), width=1)\n\n        # Function to draw centered cells\n        def draw_centered_cell(x, y, color, size_factor=0.6):\n            size = int(cell_size * size_factor)\n            center_x = x + cell_size // 2\n            center_y = y + cell_size // 2\n            top_left_x = center_x - size // 2\n            top_left_y = center_y - size // 2\n            draw.rectangle([top_left_x, top_left_y, \n                            top_left_x + size, top_left_y + size], \n                        fill=color)\n\n        # Draw start (green), goal (red), and agent (blue)\n        start_x, start_y = offset_x + self.start[1] * cell_size, offset_y + self.start[0] * cell_size\n        goal_x, goal_y = offset_x + self.goal[1] * cell_size, offset_y + self.goal[0] * cell_size\n        agent_x, agent_y = offset_x + self.state[1] * cell_size, offset_y + self.state[0] * cell_size\n\n        draw_centered_cell(start_x, start_y, (0, 255, 0))  # Green start\n        draw_centered_cell(goal_x, goal_y, (255, 0, 0))    # Red goal\n        \n        # Draw agent as a centered circle\n        agent_size = int(cell_size * 0.4)\n        center_x = agent_x + cell_size // 2\n        center_y = agent_y + cell_size // 2\n        top_left_x = center_x - agent_size // 2\n        top_left_y = center_y - agent_size // 2\n        draw.ellipse([top_left_x, top_left_y, \n                    top_left_x + agent_size, top_left_y + agent_size], \n                    fill=(0, 0, 255))  # Blue agent\n\n        return np.array(img)\n\n    def render(self, mode='human'):\n        img = self._render_maze()\n        if mode == 'human':\n            Image.fromarray(img).show()\n        elif mode == 'rgb_array':\n            return img","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:12.794553Z","iopub.execute_input":"2024-07-30T06:55:12.794826Z","iopub.status.idle":"2024-07-30T06:55:13.355695Z","shell.execute_reply.started":"2024-07-30T06:55:12.794804Z","shell.execute_reply":"2024-07-30T06:55:13.354677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass CustomMazeEnvMlpImprovedML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvMlpImprovedML, self).__init__()\n        \n        \n        # Define the size of the maze\n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n        \n        # Define action and observation space\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        self.observation_space = spaces.Box(low=0, high=127, shape=(self.maze_size * self.maze_size,), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n\n        # Initial state\n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n\n        # Initialize visit counts\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n        \n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n\n    def initialize_maze(self, input_string):\n        rows = input_string.strip().split('\\n')\n        maze = np.zeros(self.maze_size * self.maze_size, dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                state_index = i * self.maze_size + j\n                actions = parsed_data[state_index]\n                \n                state = 0\n                state |= (1 if actions[3] == -10 else 0)  # Top wall\n                state |= (1 if actions[1] == -10 else 0) << 1  # Right wall\n                state |= (1 if actions[0] == -10 else 0) << 2  # Bottom wall\n                state |= (1 if actions[2] == -10 else 0) << 3  # Left wall\n                state |= (1 if (i, j) == self.start else 0) << 4  # Start position\n                state |= (1 if (i, j) == self.goal else 0) << 5  # Goal position\n                \n                maze[state_index] = state\n\n        return maze\n\n    def _get_obs(self):\n        obs = self.maze.copy()\n        x, y = self.state\n        obs[x * self.maze_size + y] |= 1 << 6  # Set the agent bit\n        return obs\n\n    def reset(self, seed=42, options=None):\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n        \n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        state_index = x * self.maze_size + y\n\n        # Check for walls\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[state_index] & 1):  # Top wall\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[state_index] & (1 << 1)):  # Right wall\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[state_index] & (1 << 2)):  # Bottom wall\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[state_index] & (1 << 3)):  # Left wall\n            wall_hit = True\n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        truncated = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n            truncated = True\n        \n        return self._get_obs(), self.reward, self.done, truncated, {\"is_success\": self.is_success}\n    \n    def render(self, mode='human'):\n        maze_render = np.full((self.maze_size * 2 + 1, self.maze_size * 2 + 1), ' ', dtype=str)\n        \n        # Draw walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                cell_y, cell_x = i * 2 + 1, j * 2 + 1\n                state_index = i * self.maze_size + j\n                state = self.maze[state_index]\n                \n                if state & 1:  # Top wall\n                    maze_render[cell_y - 1, cell_x] = '─'\n                if state & (1 << 1):  # Right wall\n                    maze_render[cell_y, cell_x + 1] = '│'\n                if state & (1 << 2):  # Bottom wall\n                    maze_render[cell_y + 1, cell_x] = '─'\n                if state & (1 << 3):  # Left wall\n                    maze_render[cell_y, cell_x - 1] = '│'\n\n        # Mark start, goal, and current position\n        start_y, start_x = 1, 1\n        goal_y, goal_x = self.maze_size * 2 - 1, self.maze_size * 2 - 1\n        agent_y, agent_x = self.state[0] * 2 + 1, self.state[1] * 2 + 1\n\n        maze_render[start_y, start_x] = 'S'\n        maze_render[goal_y, goal_x] = 'G'\n        maze_render[agent_y, agent_x] = 'A'\n\n        # Print the maze\n        for row in maze_render:\n            print(''.join(row))\n        print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:13.357659Z","iopub.execute_input":"2024-07-30T06:55:13.357941Z","iopub.status.idle":"2024-07-30T06:55:13.391223Z","shell.execute_reply.started":"2024-07-30T06:55:13.357918Z","shell.execute_reply":"2024-07-30T06:55:13.390235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass CustomMazeEnvCnnML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvCnnML, self).__init__()\n        \n        # Define the size of the maze\n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n            \n        # Define action and observation space\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        self.observation_space = spaces.Box(low=0, high=127, shape=(1, self.maze_size, self.maze_size), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n\n        # Initial state\n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n        \n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n\n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n    def initialize_maze(self, input_string):\n        maze_size = self.maze_size\n        rows = input_string.strip().split('\\n')\n\n        maze = np.zeros((maze_size, maze_size), dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n        for i in range(maze_size):\n            for j in range(maze_size):\n                state_index = i * maze_size + j\n                actions = parsed_data[state_index]\n                \n                state = 0\n                state |= (1 if actions[3] == -10 else 0)  # Top wall\n                state |= (1 if actions[1] == -10 else 0) << 1  # Right wall\n                state |= (1 if actions[0] == -10 else 0) << 2  # Bottom wall\n                state |= (1 if actions[2] == -10 else 0) << 3  # Left wall\n                state |= (1 if (i, j) == self.start else 0) << 4  # Start position\n                state |= (1 if (i, j) == self.goal else 0) << 5  # Goal position\n                \n                maze[i, j] = state\n        \n        return maze\n        \n    def _get_obs(self):\n        obs = self.maze.copy()\n        agent_x, agent_y = self.state\n        obs[agent_x, agent_y] |= 1 << 6  # Set the agent bit\n        return obs\n\n    def reset(self, seed=42, options=None):\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n        \n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        # Check for walls or boundaries\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[x, y] & 1):  # Top wall or top edge\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[x, y] & (1 << 1)):  # Right wall or right edge\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[x, y] & (1 << 2)):  # Bottom wall or bottom edge\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[x, y] & (1 << 3)):  # Left wall or left edge\n            wall_hit = True\n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n        \n        return self._get_obs(), self.reward, self.done, False, {\"is_success\": self.is_success}\n\n    \n    def _state_to_binary(self, state):\n        return f\"{state:07b}\"\n\n    def render_binary(self):\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                state = self.maze[i, j]\n                if self.state == (i, j):\n                    state |= 1 << 6  # Set agent bit if it's the current position\n                binary = self._state_to_binary(state)\n                print(f\"{binary} \", end=\"\")\n            print()  # New line after each row\n        print()  # Empty line at the end\n\n    def render(self, mode='human'):\n        maze_render = np.full((self.maze_size * 2 + 1, self.maze_size * 2 + 1), ' ', dtype=str)\n        \n        # Draw walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                cell_y, cell_x = i * 2 + 1, j * 2 + 1\n                cell_state = self.maze[i, j]\n                if cell_state & 1:  # Top wall\n                    maze_render[cell_y - 1, cell_x] = '─'\n                if cell_state & (1 << 1):  # Right wall\n                    maze_render[cell_y, cell_x + 1] = '│'\n                if cell_state & (1 << 2):  # Bottom wall\n                    maze_render[cell_y + 1, cell_x] = '─'\n                if cell_state & (1 << 3):  # Left wall\n                    maze_render[cell_y, cell_x - 1] = '│'\n\n        # Mark start, goal, and current position\n        start_y, start_x = 1, 1\n        goal_y, goal_x = self.maze_size * 2 - 1, self.maze_size * 2 - 1\n        agent_y, agent_x = self.state[0] * 2 + 1, self.state[1] * 2 + 1\n\n        maze_render[start_y, start_x] = 'S'\n        maze_render[goal_y, goal_x] = 'G'\n        maze_render[agent_y, agent_x] = 'A'\n\n        # Print the maze\n        for row in maze_render:\n            print(''.join(row))\n        print(\"\\n\")\n\n        # Print binary representation\n        print(\"Binary representation of the maze state:\")\n        self.render_binary()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:27:04.397050Z","iopub.execute_input":"2024-07-30T12:27:04.397364Z","iopub.status.idle":"2024-07-30T12:27:04.897486Z","shell.execute_reply.started":"2024-07-30T12:27:04.397340Z","shell.execute_reply":"2024-07-30T12:27:04.896472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass CustomMazeEnvMlpML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvMlpML, self).__init__()\n        \n        \n        # Define the size of the maze\n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n        \n        # Define action and observation space\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        self.observation_space = spaces.Box(low=0, high=1, shape=(self.maze_size * self.maze_size,), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n\n        # Initial state\n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n\n        # Initialize visit counts\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n\n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n\n    def initialize_maze(self, input_string):\n        maze_size = self.maze_size\n        rows = input_string.strip().split('\\n')\n\n        top_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        right_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        bottom_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        left_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n\n        for i in range(maze_size):\n            for j in range(maze_size):\n                state_index = i * maze_size + j\n                actions = parsed_data[state_index]\n                bottom_wall[i, j] = 1 if actions[0] == -10 else 0\n                right_wall[i, j] = 1 if actions[1] == -10 else 0\n                left_wall[i, j] = 1 if actions[2] == -10 else 0\n                top_wall[i, j] = 1 if actions[3] == -10 else 0\n\n        return np.stack([top_wall, right_wall, bottom_wall, left_wall], axis=0)\n\n    def _get_obs(self):\n        obs = np.zeros(self.maze_size * self.maze_size, dtype=np.uint8)\n        obs[self.state[0] * self.maze_size + self.state[1]] = 1  # Agent position\n        return obs\n\n    def reset(self, seed=42, options=None):\n        # Reset the state of the environment to an initial state\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n\n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        \n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        # Check for walls or boundaries\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[0, x, y] == 1):  # Top wall or top edge\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[1, x, y] == 1):  # Right wall or right edge\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[2, x, y] == 1):  # Bottom wall or bottom edge\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[3, x, y] == 1):  # Left wall or left edge\n            wall_hit = True\n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n        \n        return self._get_obs(), self.reward, self.done, False, {\"is_success\": self.is_success}\n    \n    def render(self, mode='human'):\n        maze_render = np.full((self.maze_size * 2 + 1, self.maze_size * 2 + 1), ' ', dtype=str)\n        \n        # Draw walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                cell_y, cell_x = i * 2 + 1, j * 2 + 1\n                if self.maze[0, i, j]:  # Top wall\n                    maze_render[cell_y - 1, cell_x] = '─'\n                if self.maze[1, i, j]:  # Right wall\n                    maze_render[cell_y, cell_x + 1] = '│'\n                if self.maze[2, i, j]:  # Bottom wall\n                    maze_render[cell_y + 1, cell_x] = '─'\n                if self.maze[3, i, j]:  # Left wall\n                    maze_render[cell_y, cell_x - 1] = '│'\n\n        # Mark start, goal, and current position\n        start_y, start_x = 1, 1\n        goal_y, goal_x = self.maze_size * 2 - 1, self.maze_size * 2 - 1\n        agent_y, agent_x = self.state[0] * 2 + 1, self.state[1] * 2 + 1\n\n        maze_render[start_y, start_x] = 'S'\n        maze_render[goal_y, goal_x] = 'G'\n        maze_render[agent_y, agent_x] = 'A'\n\n        # Print the maze\n        for row in maze_render:\n            print(''.join(row))\n        print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:13.429381Z","iopub.execute_input":"2024-07-30T06:55:13.429659Z","iopub.status.idle":"2024-07-30T06:55:13.462087Z","shell.execute_reply.started":"2024-07-30T06:55:13.429630Z","shell.execute_reply":"2024-07-30T06:55:13.461129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch as th\nimport torch.nn as nn\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nimport gymnasium as gym\n\nclass CustomCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        \n        # The input will now have the channel dimension first\n        n_input_channels = observation_space.shape[0]\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.ReLU(),\n            nn.Linear(features_dim, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\n# Create the DQN model with CustomCNN\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n    features_extractor_kwargs=dict(features_dim=64),\n    net_arch=[64]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:13.463026Z","iopub.execute_input":"2024-07-30T06:55:13.463278Z","iopub.status.idle":"2024-07-30T06:55:29.372855Z","shell.execute_reply.started":"2024-07-30T06:55:13.463257Z","shell.execute_reply":"2024-07-30T06:55:29.371831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the environment\n# env = CustomMazeEnvMlp(maze_data=MAZE_DATA)\n# env = CustomMazeEnvCNNIMG(maze_data=MAZE_DATA)\n# env = CustomMazeEnv(maze_data=MAZE_DATA)\n# env = CustomMazeEnvMlpImproved(maze_data=MAZE_DATA)\nenv = CustomMazeEnvMlpML(maze_data_list=maze_data_list_alot, max_steps_per_episode=1, maze_size=10, random_start=True)\n\n# Verify the environment\nprint(\"Observation space:\", env.observation_space)\nprint(\"Action space:\", env.action_space)\n\n# Test the environment with random actions\nobs, _ = env.reset(42)\nfor _ in range(10):\n    # action = env.action_space.sample()  # take a random action\n    action = 1\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(\"State:\", env.state, \"Action:\", action, \"Reward:\", reward, \"Terminated:\", terminated, \"Truncated:\", truncated)\n    env.render()\n    # env.render_binary()\n    print(obs)\n    if terminated or truncated:\n        obs, _ = env.reset()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:29.374183Z","iopub.execute_input":"2024-07-30T06:55:29.374856Z","iopub.status.idle":"2024-07-30T06:55:29.414382Z","shell.execute_reply.started":"2024-07-30T06:55:29.374823Z","shell.execute_reply":"2024-07-30T06:55:29.413554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"3e0201b52f7e9c9458608859c4e12c2d7b64a9b8\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:29.415510Z","iopub.execute_input":"2024-07-30T06:55:29.418016Z","iopub.status.idle":"2024-07-30T06:55:32.559645Z","shell.execute_reply.started":"2024-07-30T06:55:29.417980Z","shell.execute_reply":"2024-07-30T06:55:32.558789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport torch\nfrom stable_baselines3.common.callbacks import BaseCallback\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass WandbCallbackMod(BaseCallback):\n    def __init__(self, verbose=0):\n        super().__init__(verbose)\n        self.cumulative_reward = 0\n        self.episode_reward = 0\n        self.success_count = 0\n        self.episode_count = 0\n\n    def _on_step(self):\n        # Log all available metrics from the logger\n        for key, value in self.model.logger.name_to_value.items():\n            wandb.log({key: value}, step=self.num_timesteps)\n        \n        reward = self.model.env.get_attr('reward')[0]\n        \n        self.cumulative_reward += reward\n        self.episode_reward += reward\n        \n        if self.model.env.get_attr('done')[0]:\n            # Increment the episode count\n            self.episode_count += 1\n            \n            # If the episode was successful, increment the success count\n            if self.model.env.get_attr('is_success')[0]:\n                self.success_count += 1\n            \n            wandb.log({\n                'train/cumulative_reward': self.cumulative_reward,\n                'train/episode_reward': self.episode_reward,\n                'train/success_rate': self.success_count / self.episode_count\n            }, step=self.num_timesteps)\n            \n            # Reset the episode reward\n            self.episode_reward = 0\n\n        return True\n    \n    def _on_training_end(self):\n        wandb.finish()\n        \n        return None","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:32.560613Z","iopub.execute_input":"2024-07-30T06:55:32.561129Z","iopub.status.idle":"2024-07-30T06:55:32.593768Z","shell.execute_reply.started":"2024-07-30T06:55:32.561104Z","shell.execute_reply":"2024-07-30T06:55:32.592760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3 import DQN\n\nenv_mlp_ml = CustomMazeEnvMlpML(maze_data_list=maze_data_list_alot, max_steps_per_episode=400, maze_size=10, random_start=True)\n\nmodel_mlp_ml = DQN(\n    \"MlpPolicy\",\n    env_mlp_ml,\n    learning_rate=1e-4,\n    buffer_size=1000,\n    learning_starts=100,\n    batch_size=32,\n    gamma=0.85,\n    target_update_interval=5,\n    exploration_fraction = 0.8,\n    exploration_initial_eps=1.0,\n    exploration_final_eps=0.5,\n    device=device,\n    verbose=1,\n    seed=42,\n)\n\ncallback = WandbCallbackMod()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:32.594941Z","iopub.execute_input":"2024-07-30T06:55:32.595216Z","iopub.status.idle":"2024-07-30T06:55:34.140200Z","shell.execute_reply.started":"2024-07-30T06:55:32.595191Z","shell.execute_reply":"2024-07-30T06:55:34.139426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"maze-dqn\", name=\"stable-mlp-maze-10-3-testcount\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:34.142647Z","iopub.execute_input":"2024-07-30T06:55:34.142929Z","iopub.status.idle":"2024-07-30T06:55:51.354400Z","shell.execute_reply.started":"2024-07-30T06:55:34.142905Z","shell.execute_reply":"2024-07-30T06:55:51.353451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_mlp_ml.learn(total_timesteps=2000000, callback=[callback], log_interval=1000)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:55:51.355793Z","iopub.execute_input":"2024-07-30T06:55:51.356154Z","iopub.status.idle":"2024-07-30T06:56:20.694191Z","shell.execute_reply.started":"2024-07-30T06:55:51.356117Z","shell.execute_reply":"2024-07-30T06:56:20.692594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_mlp_ml.save(\"stable-mlp-maze-10-3-testcount\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T06:56:20.695176Z","iopub.status.idle":"2024-07-30T06:56:20.695716Z","shell.execute_reply.started":"2024-07-30T06:56:20.695450Z","shell.execute_reply":"2024-07-30T06:56:20.695470Z"},"trusted":true},"execution_count":null,"outputs":[]}]}