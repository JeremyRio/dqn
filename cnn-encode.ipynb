{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90600,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":75948,"modelId":100659}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"maze_data_list_alot = [\n    \"\"\"\n    -10;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-10;\n    -1;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -10;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -10;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-10;\n    -10;-1;-1;-1;\n    -10;-1;-1;-1;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    \"\"\",\n    \"\"\"\n    -1;-10;-10;-10;\n    -1;-1;-10;-10;\n    -1;-1;-1;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -1;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -1;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -10;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -10;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    \"\"\",\n    \"\"\"\n    -1;-1;-10;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-1;-1;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-1;\n    -10;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-10;\n    -10;-1;-10;-1;\n    -1;-10;-1;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-1;\n    -10;-10;-1;-10;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-1;\n    -1;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-1;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -1;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -1;-10;-10;-1;\n    -1;-1;-10;-1;\n    -1;-10;-1;-10;\n    -10;-1;-10;-10;\n    -1;-1;-1;-10;\n    -10;-10;-1;-10;\n    -1;-1;-10;-10;\n    -1;-10;-1;-10;\n    -1;-1;-10;-10;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-10;-10;-1;\n    -10;-1;-10;-1;\n    -10;-10;-1;-10;\n    -10;-1;-10;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-1;\n    -10;-1;-10;-1;\n    -10;-1;-1;-1;\n    -10;-1;-1;-10;\n    -10;-10;-1;-10;\n    \"\"\",\n#     \"\"\"\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-10;\n#     -1;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -1;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -1;-10;-1;-1;\n#     -1;-10;-10;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-1;\n#     -10;-1;-1;-1;\n#     -10;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-10;\n#     \"\"\",\n#     \"\"\"\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-1;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-1;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-1;\n#     -10;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-10;-10;-10;\n#     -1;-1;-10;-10;\n#     -10;-1;-1;-1;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -1;-10;-1;-10;\n#     -1;-1;-10;-1;\n#     -1;-10;-1;-10;\n#     -1;-10;-10;-1;\n#     -10;-1;-10;-10;\n#     -10;-10;-1;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-10;\n#     -10;-10;-10;-1;\n#     -1;-1;-10;-10;\n#     -1;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-1;\n#     -10;-1;-1;-10;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-1;-10;-1;\n#     -10;-1;-1;-10;\n#     -10;-10;-1;-1;\n#     -10;-10;-10;-1;\n#     \"\"\",\n#     \"\"\"\n#     -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -1;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n#     \"\"\",\n# \"\"\"\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -10;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-1;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# \"\"\",\n# \"\"\"\n# -1;-1;-10;-10;\n# -1;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -1;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-1;-10;-1;\n# -1;-1;-1;-10;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# \"\"\",\n# \"\"\"\n# -10;-1;-10;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -1;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-10;\n# -1;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -10;-1;-1;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-1;-1;-10;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# \"\"\",\n# \"\"\"\n# -1;-10;-10;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-1;-10;-1;\n# -10;-1;-1;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -1;-10;-10;-10;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -1;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -1;-10;-1;-10;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -1;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-10;-1;-10;\n# -1;-10;-10;-1;\n# -10;-1;-10;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-10;-1;-1;\n# -1;-10;-10;-1;\n# -10;-10;-10;-1;\n# -1;-10;-10;-1;\n# -1;-1;-10;-10;\n# -10;-1;-1;-10;\n# -1;-10;-1;-1;\n# -10;-1;-10;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-1;-1;-10;\n# -10;-1;-1;-1;\n# -10;-10;-1;-1;\n# -10;-1;-10;-10;\n# -10;-10;-1;-1;\n# \"\"\"\n]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.461254Z","iopub.execute_input":"2024-08-07T07:13:36.461612Z","iopub.status.idle":"2024-08-07T07:13:36.485930Z","shell.execute_reply.started":"2024-08-07T07:13:36.461583Z","shell.execute_reply":"2024-08-07T07:13:36.484733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom PIL import Image, ImageDraw\n\nclass CustomMazeEnvCNNIMGML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvCNNIMGML, self).__init__()\n        \n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        \n        # Define image size for DQN\n        self.image_size = 84\n        self.observation_space = spaces.Box(low=0, high=255, shape=(self.image_size, self.image_size, 3), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n        \n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n\n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n        \n    def initialize_maze(self, input_string):\n        maze_size = self.maze_size\n        rows = input_string.strip().split('\\n')\n        top_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        right_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        bottom_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        left_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n        for i in range(maze_size):\n            for j in range(maze_size):\n                state_index = i * maze_size + j\n                actions = parsed_data[state_index]\n                bottom_wall[i, j] = 1 if actions[0] == -10 else 0\n                right_wall[i, j] = 1 if actions[1] == -10 else 0\n                left_wall[i, j] = 1 if actions[2] == -10 else 0\n                top_wall[i, j] = 1 if actions[3] == -10 else 0\n\n        return np.stack([top_wall, right_wall, bottom_wall, left_wall], axis=0)\n        \n    def _get_obs(self):\n        return self._render_maze()\n\n    def reset(self, seed=42, options=None):\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n        \n        # Change to the next maze\n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        # Check for walls or boundaries\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[0, x, y] == 1):  # Top wall or top edge\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[1, x, y] == 1):  # Right wall or right edge\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[2, x, y] == 1):  # Bottom wall or bottom edge\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[3, x, y] == 1):  # Left wall or left edge\n            wall_hit = True\n        \n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n        \n        return self._get_obs(), self.reward, self.done, False, {\"is_success\": self.is_success}\n\n    \n    def _render_maze(self):\n        img = Image.new('RGB', (self.image_size, self.image_size), color=(255, 255, 255))\n        draw = ImageDraw.Draw(img)\n\n        cell_size = min(self.image_size // self.maze_size, self.image_size // self.maze_size)\n        maze_pixel_size = cell_size * self.maze_size\n        offset_x = (self.image_size - maze_pixel_size) // 2\n        offset_y = (self.image_size - maze_pixel_size) // 2\n\n        # Draw the maze walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                x, y = offset_x + j * cell_size, offset_y + i * cell_size\n                walls = [(0, [(x, y), (x + cell_size, y)]),             # Top\n                        (1, [(x + cell_size, y), (x + cell_size, y + cell_size)]),  # Right\n                        (2, [(x, y + cell_size), (x + cell_size, y + cell_size)]),  # Bottom\n                        (3, [(x, y), (x, y + cell_size)])]            # Left\n                for idx, line in walls:\n                    if self.maze[idx, i, j]:\n                        draw.line(line, fill=(0, 0, 0), width=1)\n\n        # Function to draw centered cells\n        def draw_centered_cell(x, y, color, size_factor=0.6):\n            size = int(cell_size * size_factor)\n            center_x = x + cell_size // 2\n            center_y = y + cell_size // 2\n            top_left_x = center_x - size // 2\n            top_left_y = center_y - size // 2\n            draw.rectangle([top_left_x, top_left_y, \n                            top_left_x + size, top_left_y + size], \n                        fill=color)\n\n        # Draw start (green), goal (red), and agent (blue)\n        start_x, start_y = offset_x + self.start[1] * cell_size, offset_y + self.start[0] * cell_size\n        goal_x, goal_y = offset_x + self.goal[1] * cell_size, offset_y + self.goal[0] * cell_size\n        agent_x, agent_y = offset_x + self.state[1] * cell_size, offset_y + self.state[0] * cell_size\n\n        draw_centered_cell(start_x, start_y, (0, 255, 0))  # Green start\n        draw_centered_cell(goal_x, goal_y, (255, 0, 0))    # Red goal\n        \n        # Draw agent as a centered circle\n        agent_size = int(cell_size * 0.4)\n        center_x = agent_x + cell_size // 2\n        center_y = agent_y + cell_size // 2\n        top_left_x = center_x - agent_size // 2\n        top_left_y = center_y - agent_size // 2\n        draw.ellipse([top_left_x, top_left_y, \n                    top_left_x + agent_size, top_left_y + agent_size], \n                    fill=(0, 0, 255))  # Blue agent\n\n        return np.array(img)\n\n    def render(self, mode='human'):\n        img = self._render_maze()\n        if mode == 'human':\n            Image.fromarray(img).show()\n        elif mode == 'rgb_array':\n            return img","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.488298Z","iopub.execute_input":"2024-08-07T07:13:36.488892Z","iopub.status.idle":"2024-08-07T07:13:36.529873Z","shell.execute_reply.started":"2024-08-07T07:13:36.488855Z","shell.execute_reply":"2024-08-07T07:13:36.528838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass CustomMazeEnvMlpImprovedML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvMlpImprovedML, self).__init__()\n        \n        \n        # Define the size of the maze\n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n        \n        # Define action and observation space\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        self.observation_space = spaces.Box(low=0, high=127, shape=(self.maze_size * self.maze_size,), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n\n        # Initial state\n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n\n        # Initialize visit counts\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n        \n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n\n    def initialize_maze(self, input_string):\n        rows = input_string.strip().split('\\n')\n        maze = np.zeros(self.maze_size * self.maze_size, dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                state_index = i * self.maze_size + j\n                actions = parsed_data[state_index]\n                \n                state = 0\n                state |= (1 if actions[3] == -10 else 0)  # Top wall\n                state |= (1 if actions[1] == -10 else 0) << 1  # Right wall\n                state |= (1 if actions[0] == -10 else 0) << 2  # Bottom wall\n                state |= (1 if actions[2] == -10 else 0) << 3  # Left wall\n                state |= (1 if (i, j) == self.start else 0) << 4  # Start position\n                state |= (1 if (i, j) == self.goal else 0) << 5  # Goal position\n                \n                maze[state_index] = state\n\n        return maze\n\n    def _get_obs(self):\n        obs = self.maze.copy()\n        x, y = self.state\n        obs[x * self.maze_size + y] |= 1 << 6  # Set the agent bit\n        return obs\n\n    def reset(self, seed=42, options=None):\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n        \n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        state_index = x * self.maze_size + y\n\n        # Check for walls\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[state_index] & 1):  # Top wall\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[state_index] & (1 << 1)):  # Right wall\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[state_index] & (1 << 2)):  # Bottom wall\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[state_index] & (1 << 3)):  # Left wall\n            wall_hit = True\n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        truncated = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n            truncated = True\n        \n        return self._get_obs(), self.reward, self.done, truncated, {\"is_success\": self.is_success}\n    \n    def render(self, mode='human'):\n        maze_render = np.full((self.maze_size * 2 + 1, self.maze_size * 2 + 1), ' ', dtype=str)\n        \n        # Draw walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                cell_y, cell_x = i * 2 + 1, j * 2 + 1\n                state_index = i * self.maze_size + j\n                state = self.maze[state_index]\n                \n                if state & 1:  # Top wall\n                    maze_render[cell_y - 1, cell_x] = '─'\n                if state & (1 << 1):  # Right wall\n                    maze_render[cell_y, cell_x + 1] = '│'\n                if state & (1 << 2):  # Bottom wall\n                    maze_render[cell_y + 1, cell_x] = '─'\n                if state & (1 << 3):  # Left wall\n                    maze_render[cell_y, cell_x - 1] = '│'\n\n        # Mark start, goal, and current position\n        start_y, start_x = 1, 1\n        goal_y, goal_x = self.maze_size * 2 - 1, self.maze_size * 2 - 1\n        agent_y, agent_x = self.state[0] * 2 + 1, self.state[1] * 2 + 1\n\n        maze_render[start_y, start_x] = 'S'\n        maze_render[goal_y, goal_x] = 'G'\n        maze_render[agent_y, agent_x] = 'A'\n\n        # Print the maze\n        for row in maze_render:\n            print(''.join(row))\n        print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.657169Z","iopub.execute_input":"2024-08-07T07:13:36.657541Z","iopub.status.idle":"2024-08-07T07:13:36.693504Z","shell.execute_reply.started":"2024-08-07T07:13:36.657510Z","shell.execute_reply":"2024-08-07T07:13:36.692343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass CustomMazeEnvCnnML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvCnnML, self).__init__()\n        \n        # Define the size of the maze\n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n            \n        # Define action and observation space\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        self.observation_space = spaces.Box(low=0, high=127, shape=(1, self.maze_size, self.maze_size), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n\n        # Initial state\n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n        \n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n\n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n    def initialize_maze(self, input_string):\n        maze_size = self.maze_size\n        rows = input_string.strip().split('\\n')\n\n        maze = np.zeros((maze_size, maze_size), dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n        for i in range(maze_size):\n            for j in range(maze_size):\n                state_index = i * maze_size + j\n                actions = parsed_data[state_index]\n                \n                state = 0\n                state |= (1 if actions[3] == -10 else 0)  # Top wall\n                state |= (1 if actions[1] == -10 else 0) << 1  # Right wall\n                state |= (1 if actions[0] == -10 else 0) << 2  # Bottom wall\n                state |= (1 if actions[2] == -10 else 0) << 3  # Left wall\n                state |= (1 if (i, j) == self.start else 0) << 4  # Start position\n                state |= (1 if (i, j) == self.goal else 0) << 5  # Goal position\n                \n                maze[i, j] = state\n        \n        return maze\n        \n    def _get_obs(self):\n        obs = self.maze.copy()\n        agent_x, agent_y = self.state\n        obs[agent_x, agent_y] |= 1 << 6  # Set the agent bit\n        return obs\n\n    def reset(self, seed=42, options=None):\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n        \n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        # Check for walls or boundaries\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[x, y] & 1):  # Top wall or top edge\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[x, y] & (1 << 1)):  # Right wall or right edge\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[x, y] & (1 << 2)):  # Bottom wall or bottom edge\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[x, y] & (1 << 3)):  # Left wall or left edge\n            wall_hit = True\n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n        \n        return self._get_obs(), self.reward, self.done, False, {\"is_success\": self.is_success}\n\n    \n    def _state_to_binary(self, state):\n        return f\"{state:07b}\"\n\n    def render_binary(self):\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                state = self.maze[i, j]\n                if self.state == (i, j):\n                    state |= 1 << 6  # Set agent bit if it's the current position\n                binary = self._state_to_binary(state)\n                print(f\"{binary} \", end=\"\")\n            print()  # New line after each row\n        print()  # Empty line at the end\n\n    def render(self, mode='human'):\n        maze_render = np.full((self.maze_size * 2 + 1, self.maze_size * 2 + 1), ' ', dtype=str)\n        \n        # Draw walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                cell_y, cell_x = i * 2 + 1, j * 2 + 1\n                cell_state = self.maze[i, j]\n                if cell_state & 1:  # Top wall\n                    maze_render[cell_y - 1, cell_x] = '─'\n                if cell_state & (1 << 1):  # Right wall\n                    maze_render[cell_y, cell_x + 1] = '│'\n                if cell_state & (1 << 2):  # Bottom wall\n                    maze_render[cell_y + 1, cell_x] = '─'\n                if cell_state & (1 << 3):  # Left wall\n                    maze_render[cell_y, cell_x - 1] = '│'\n\n        # Mark start, goal, and current position\n        start_y, start_x = 1, 1\n        goal_y, goal_x = self.maze_size * 2 - 1, self.maze_size * 2 - 1\n        agent_y, agent_x = self.state[0] * 2 + 1, self.state[1] * 2 + 1\n\n        maze_render[start_y, start_x] = 'S'\n        maze_render[goal_y, goal_x] = 'G'\n        maze_render[agent_y, agent_x] = 'A'\n\n        # Print the maze\n        for row in maze_render:\n            print(''.join(row))\n        print(\"\\n\")\n\n        # Print binary representation\n        print(\"Binary representation of the maze state:\")\n        self.render_binary()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.696156Z","iopub.execute_input":"2024-08-07T07:13:36.696559Z","iopub.status.idle":"2024-08-07T07:13:36.747006Z","shell.execute_reply.started":"2024-08-07T07:13:36.696522Z","shell.execute_reply":"2024-08-07T07:13:36.746255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass CustomMazeEnvMlpML(gym.Env):\n    def __init__(self, max_steps_per_episode=100, maze_data_list=None, maze_size=10, random_start=False):\n        super(CustomMazeEnvMlpML, self).__init__()\n        \n        \n        # Define the size of the maze\n        self.maze_size = maze_size\n        self.max_steps_per_episode = max_steps_per_episode\n        \n        # Define action and observation space\n        self.action_space = spaces.Discrete(4)  # [0: up, 1: right, 2: down, 3: left]\n        self.observation_space = spaces.Box(low=0, high=1, shape=(self.maze_size * self.maze_size,), dtype=np.uint8)\n\n        self.mazes = []\n        self.current_maze_index = 0\n        self.random_start = random_start\n        self.episode = 0\n        \n        self.reward = 0\n        self.done = False\n        self.is_success = False\n\n        # Initial state\n        self.state = None\n        self.start = None\n        self.goal = (self.maze_size - 1, self.maze_size - 1)\n\n        # Initialize visit counts\n        self.visit_counts = np.zeros((self.maze_size, self.maze_size), dtype=np.int32)\n        self.current_step = 0\n\n        for maze_data in maze_data_list:\n            self.mazes.append(self.initialize_maze(maze_data))\n        \n        self.maze = self.mazes[self.current_maze_index]\n        \n\n    def initialize_maze(self, input_string):\n        maze_size = self.maze_size\n        rows = input_string.strip().split('\\n')\n\n        top_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        right_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        bottom_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n        left_wall = np.zeros((maze_size, maze_size), dtype=np.uint8)\n\n        parsed_data = []\n        for row in rows:\n            parsed_data.append([int(x) for x in row.split(';') if x != ''])\n\n\n        for i in range(maze_size):\n            for j in range(maze_size):\n                state_index = i * maze_size + j\n                actions = parsed_data[state_index]\n                bottom_wall[i, j] = 1 if actions[0] == -10 else 0\n                right_wall[i, j] = 1 if actions[1] == -10 else 0\n                left_wall[i, j] = 1 if actions[2] == -10 else 0\n                top_wall[i, j] = 1 if actions[3] == -10 else 0\n\n        return np.stack([top_wall, right_wall, bottom_wall, left_wall], axis=0)\n\n    def _get_obs(self):\n        obs = np.zeros(self.maze_size * self.maze_size, dtype=np.uint8)\n        obs[self.state[0] * self.maze_size + self.state[1]] = 1  # Agent position\n        return obs\n\n    def reset(self, seed=42, options=None):\n        # Reset the state of the environment to an initial state\n        new_seed = seed + self.episode\n        super().reset(seed=new_seed)\n        self.episode += 1\n\n        if not self.random_start:\n            self.start = (0, 0)\n        else:\n            self.start = (self.np_random.integers(0, self.maze_size), self.np_random.integers(0, self.maze_size))\n        \n        self.current_step = 0\n        self.state = self.start\n        self.visit_counts[self.state] += 1\n\n        self.current_maze_index = (self.current_maze_index + 1) % len(self.mazes)\n        self.maze = self.mazes[self.current_maze_index]\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.current_step += 1\n        \n        x, y = self.state\n        new_x, new_y = x, y\n\n        # Define possible moves\n        if action == 0:  # up\n            new_x = max(0, x - 1)\n        elif action == 1:  # right\n            new_y = min(self.maze_size - 1, y + 1)\n        elif action == 2:  # down\n            new_x = min(self.maze_size - 1, x + 1)\n        elif action == 3:  # left\n            new_y = max(0, y - 1)\n\n        # Check for walls or boundaries\n        wall_hit = False\n        if action == 0 and (x == 0 or self.maze[0, x, y] == 1):  # Top wall or top edge\n            wall_hit = True\n        elif action == 1 and (y == self.maze_size - 1 or self.maze[1, x, y] == 1):  # Right wall or right edge\n            wall_hit = True\n        elif action == 2 and (x == self.maze_size - 1 or self.maze[2, x, y] == 1):  # Bottom wall or bottom edge\n            wall_hit = True\n        elif action == 3 and (y == 0 or self.maze[3, x, y] == 1):  # Left wall or left edge\n            wall_hit = True\n        \n        # Determine reward and update state\n        if wall_hit:\n            self.reward = -10\n        else:\n            self.state = (new_x, new_y)\n            if self.state == self.goal:\n                self.reward = 10\n            else:\n                self.reward = -1  # Default step cost\n        \n        self.visit_counts[self.state] += 1\n\n        # Check if goal is reached or max steps are reached\n        self.done = False\n        self.is_success = False\n        if self.state == self.goal:\n            self.done = True\n            self.is_success = True\n        elif self.current_step >= self.max_steps_per_episode:\n            self.done = True\n            self.is_success = False\n        \n        return self._get_obs(), self.reward, self.done, False, {\"is_success\": self.is_success}\n    \n    def render(self, mode='human'):\n        maze_render = np.full((self.maze_size * 2 + 1, self.maze_size * 2 + 1), ' ', dtype=str)\n        \n        # Draw walls\n        for i in range(self.maze_size):\n            for j in range(self.maze_size):\n                cell_y, cell_x = i * 2 + 1, j * 2 + 1\n                if self.maze[0, i, j]:  # Top wall\n                    maze_render[cell_y - 1, cell_x] = '─'\n                if self.maze[1, i, j]:  # Right wall\n                    maze_render[cell_y, cell_x + 1] = '│'\n                if self.maze[2, i, j]:  # Bottom wall\n                    maze_render[cell_y + 1, cell_x] = '─'\n                if self.maze[3, i, j]:  # Left wall\n                    maze_render[cell_y, cell_x - 1] = '│'\n\n        # Mark start, goal, and current position\n        start_y, start_x = 1, 1\n        goal_y, goal_x = self.maze_size * 2 - 1, self.maze_size * 2 - 1\n        agent_y, agent_x = self.state[0] * 2 + 1, self.state[1] * 2 + 1\n\n        maze_render[start_y, start_x] = 'S'\n        maze_render[goal_y, goal_x] = 'G'\n        maze_render[agent_y, agent_x] = 'A'\n\n        # Print the maze\n        for row in maze_render:\n            print(''.join(row))\n        print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.748256Z","iopub.execute_input":"2024-08-07T07:13:36.748814Z","iopub.status.idle":"2024-08-07T07:13:36.792136Z","shell.execute_reply.started":"2024-08-07T07:13:36.748782Z","shell.execute_reply":"2024-08-07T07:13:36.791087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch as th\nimport torch.nn as nn\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nimport gymnasium as gym\n\nclass CustomCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        \n        # The input will now have the channel dimension first\n        n_input_channels = observation_space.shape[0]\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=(3, 3), stride=(1, 1)),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n\n        # Compute shape by doing one forward pass\n        with th.no_grad():\n            n_flatten = self.cnn(\n                th.as_tensor(observation_space.sample()[None]).float()\n            ).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.ReLU(),\n            nn.Linear(features_dim, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        return self.linear(self.cnn(observations))\n\n# Create the DQN model with CustomCNN\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n    features_extractor_kwargs=dict(features_dim=64),\n    net_arch=[64]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.794872Z","iopub.execute_input":"2024-08-07T07:13:36.795725Z","iopub.status.idle":"2024-08-07T07:13:36.808354Z","shell.execute_reply.started":"2024-08-07T07:13:36.795671Z","shell.execute_reply":"2024-08-07T07:13:36.807484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the environment\n# env = CustomMazeEnvMlp(maze_data=MAZE_DATA)\n# env = CustomMazeEnvCNNIMG(maze_data=MAZE_DATA)\n# env = CustomMazeEnv(maze_data=MAZE_DATA)\n# env = CustomMazeEnvMlpImproved(maze_data=MAZE_DATA)\nenv = CustomMazeEnvCNNIMGML(maze_data_list=maze_data_list_alot, max_steps_per_episode=1, maze_size=10, random_start=True)\n\n# Verify the environment\nprint(\"Observation space:\", env.observation_space)\nprint(\"Action space:\", env.action_space)\n\n# Test the environment with random actions\nobs, _ = env.reset(42)\nfor _ in range(10):\n    # action = env.action_space.sample()  # take a random action\n    action = 1\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(\"State:\", env.state, \"Action:\", action, \"Reward:\", reward, \"Terminated:\", terminated, \"Truncated:\", truncated)\n    env.render()\n    # env.render_binary()\n    print(obs)\n    if terminated or truncated:\n        obs, _ = env.reset()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.809709Z","iopub.execute_input":"2024-08-07T07:13:36.810159Z","iopub.status.idle":"2024-08-07T07:13:36.996723Z","shell.execute_reply.started":"2024-08-07T07:13:36.810126Z","shell.execute_reply":"2024-08-07T07:13:36.995599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"3e0201b52f7e9c9458608859c4e12c2d7b64a9b8\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:36.998830Z","iopub.execute_input":"2024-08-07T07:13:37.000057Z","iopub.status.idle":"2024-08-07T07:13:40.003139Z","shell.execute_reply.started":"2024-08-07T07:13:36.999994Z","shell.execute_reply":"2024-08-07T07:13:40.002352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport torch\nfrom stable_baselines3.common.callbacks import BaseCallback\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass WandbCallbackMod(BaseCallback):\n    def __init__(self, verbose=0):\n        super().__init__(verbose)\n        self.cumulative_reward = 0\n        self.episode_reward = 0\n        self.success_count = 0\n        self.episode_count = 0\n\n    def _on_step(self):\n        # Log all available metrics from the logger\n        for key, value in self.model.logger.name_to_value.items():\n            wandb.log({key: value}, step=self.num_timesteps)\n        \n        reward = self.model.env.get_attr('reward')[0]\n        \n        self.cumulative_reward += reward\n        self.episode_reward += reward\n        \n        if self.model.env.get_attr('done')[0]:\n            # Increment the episode count\n            self.episode_count += 1\n            \n            # If the episode was successful, increment the success count\n            if self.model.env.get_attr('is_success')[0]:\n                self.success_count += 1\n            \n            wandb.log({\n                'train/cumulative_reward': self.cumulative_reward,\n                'train/episode_reward': self.episode_reward,\n                'train/success_rate': self.success_count / self.episode_count\n            }, step=self.num_timesteps)\n            \n            # Reset the episode reward\n            self.episode_reward = 0\n\n        return True\n    \n    def _on_training_end(self):\n        wandb.finish()\n        \n        return None","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:40.004297Z","iopub.execute_input":"2024-08-07T07:13:40.004926Z","iopub.status.idle":"2024-08-07T07:13:40.040262Z","shell.execute_reply.started":"2024-08-07T07:13:40.004899Z","shell.execute_reply":"2024-08-07T07:13:40.039231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3 import DQN\n\nenv_img_ml = CustomMazeEnvCNNIMGML(maze_data_list=maze_data_list_alot, max_steps_per_episode=400, maze_size=10, random_start=True)\n\nmodel_img_ml = DQN(\n    \"CnnPolicy\",\n    env_img_ml,\n    learning_rate=1e-4,\n    buffer_size=1000,\n    learning_starts=100,\n    batch_size=32,\n    gamma=0.85,\n    target_update_interval=5,\n    exploration_fraction = 0.8,\n    exploration_initial_eps=1.0,\n    exploration_final_eps=0.5,\n    device=device,\n    verbose=1,\n    seed=42,\n)\n\ncallback = WandbCallbackMod()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:40.041635Z","iopub.execute_input":"2024-08-07T07:13:40.041996Z","iopub.status.idle":"2024-08-07T07:13:41.776480Z","shell.execute_reply.started":"2024-08-07T07:13:40.041969Z","shell.execute_reply":"2024-08-07T07:13:41.775684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model_img_ml.policy)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:41.777599Z","iopub.execute_input":"2024-08-07T07:13:41.777887Z","iopub.status.idle":"2024-08-07T07:13:41.782963Z","shell.execute_reply.started":"2024-08-07T07:13:41.777863Z","shell.execute_reply":"2024-08-07T07:13:41.782107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project=\"maze-dqn\", name=\"stable-image-maze-10-3-testcount\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:41.785592Z","iopub.execute_input":"2024-08-07T07:13:41.785861Z","iopub.status.idle":"2024-08-07T07:13:41.793630Z","shell.execute_reply.started":"2024-08-07T07:13:41.785838Z","shell.execute_reply":"2024-08-07T07:13:41.792744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_img_ml.learn(total_timesteps=2000000, callback=[callback], log_interval=1000)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:41.794942Z","iopub.execute_input":"2024-08-07T07:13:41.795299Z","iopub.status.idle":"2024-08-07T07:13:41.804494Z","shell.execute_reply.started":"2024-08-07T07:13:41.795268Z","shell.execute_reply":"2024-08-07T07:13:41.803503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_img_ml.save(\"stable-image-maze-10-3-testcount\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:41.805709Z","iopub.execute_input":"2024-08-07T07:13:41.806036Z","iopub.status.idle":"2024-08-07T07:13:41.814740Z","shell.execute_reply.started":"2024-08-07T07:13:41.806007Z","shell.execute_reply":"2024-08-07T07:13:41.813804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from collections import deque\n\n# def bfs_shortest_path(maze, start, goal):\n#     maze_size = maze.shape[1]\n#     queue = deque([(start, 0)])  # (position, steps)\n#     visited = set()\n#     visited.add(start)\n    \n#     directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # up, right, down, left\n    \n#     while queue:\n#         (x, y), steps = queue.popleft()\n        \n#         if (x, y) == goal:\n#             return steps\n        \n#         for i, (dx, dy) in enumerate(directions):\n#             new_x, new_y = x + dx, y + dy\n#             if 0 <= new_x < maze_size and 0 <= new_y < maze_size and (new_x, new_y) not in visited:\n#                 if i == 0 and maze[0, x, y] == 0:  # up\n#                     queue.append(((new_x, new_y), steps + 1))\n#                     visited.add((new_x, new_y))\n#                 elif i == 1 and maze[1, x, y] == 0:  # right\n#                     queue.append(((new_x, new_y), steps + 1))\n#                     visited.add((new_x, new_y))\n#                 elif i == 2 and maze[2, x, y] == 0:  # down\n#                     queue.append(((new_x, new_y), steps + 1))\n#                     visited.add((new_x, new_y))\n#                 elif i == 3 and maze[3, x, y] == 0:  # left\n#                     queue.append(((new_x, new_y), steps + 1))\n#                     visited.add((new_x, new_y))\n    \n#     return float('inf')  # Return a large number if no path is found\n","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:41.815870Z","iopub.execute_input":"2024-08-07T07:13:41.816932Z","iopub.status.idle":"2024-08-07T07:13:41.828931Z","shell.execute_reply.started":"2024-08-07T07:13:41.816895Z","shell.execute_reply":"2024-08-07T07:13:41.828004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate_agent(model, env, num_episodes=1000):\n#     successes = 0\n#     total_rewards = 0\n#     total_steps = 0\n#     seed_count = 42\n    \n#     for episode in range(num_episodes):\n#         obs, _ = env.reset(seed_count)\n#         episode_reward = 0\n#         done = False\n#         truncated = False\n#         steps = 0\n# #         shortest_path_steps = bfs_shortest_path(env.maze, env.start, env.goal)\n# #         env.render()\n        \n#         while not (done or truncated):\n#             obs = obs.reshape((1, 10, 10))\n#             action, _ = model.predict(obs, deterministic=True)\n#             obs, reward, done, truncated, info = env.step(action)\n#             episode_reward += reward\n#             steps += 1\n\n#             if done and reward > 0:  # Assuming positive reward indicates success\n#                 successes += 1\n        \n# #         wandb.define_metric(\"accuracy_rate\", step_metric=\"success\")\n# #         if done and reward > 0:\n# #             wandb.log({\n# #                 'success': successes,\n# #                 'accuracy_rate': shortest_path_steps / steps\n# #             })\n        \n#         seed_count += 1\n# #         env.render()\n#         total_rewards += episode_reward\n#         total_steps += steps\n#         wandb.log({\n#             'train/cumulative_reward': total_rewards,\n#             'train/episode_reward': episode_reward,\n#             'train/success_rate': (successes / (episode + 1)) * 100,\n#         }, step=episode + 1)\n        \n\n\n\n# #         print(f\"Episode {episode + 1}: Reward = {episode_reward}, Steps = {steps}\")\n\n#     success_rate = successes / num_episodes\n#     avg_reward = total_rewards / num_episodes\n#     avg_steps = total_steps / num_episodes\n\n#     print(f\"\\nEvaluation over {num_episodes} episodes:\")\n#     print(f\"Success Rate: {success_rate:.2f}%\")\n#     print(f\"Average Reward: {avg_reward:.2f}\")\n#     print(f\"Average Steps: {avg_steps:.2f}\")\n#     wandb.finish()\n\n#     return success_rate, avg_reward, avg_steps","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:14:27.071410Z","iopub.execute_input":"2024-08-07T07:14:27.072273Z","iopub.status.idle":"2024-08-07T07:14:27.083721Z","shell.execute_reply.started":"2024-08-07T07:14:27.072237Z","shell.execute_reply":"2024-08-07T07:14:27.082724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import zipfile\n\n# def zip_subdirectory(input_dir, output_dir):\n#     # Get the name of the subdirectory\n#     subdir_name = os.path.basename(input_dir)\n    \n#     # Create the output zip file path\n#     zip_filename = f\"{subdir_name}.zip\"\n#     zip_path = os.path.join(output_dir, zip_filename)\n    \n#     # Create a ZipFile object\n#     with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n#         # Walk through the directory\n#         for root, _, files in os.walk(input_dir):\n#             for file in files:\n#                 # Create the full file path\n#                 file_path = os.path.join(root, file)\n#                 # Calculate the archive name (path relative to input_dir)\n#                 arcname = os.path.relpath(file_path, input_dir)\n#                 # Add file to zip\n#                 zipf.write(file_path, arcname)\n    \n#     print(f\"Created zip file: {zip_path}\")\n\n# # Main execution\n# input_dir = '/kaggle/input/modeldqn/pytorch/default/1'\n# output_dir = '/kaggle/working'\n\n# # Ensure the output directory exists\n# os.makedirs(output_dir, exist_ok=True)\n\n# # Iterate through immediate subdirectories\n# for item in os.listdir(input_dir):\n#     item_path = os.path.join(input_dir, item)\n#     if os.path.isdir(item_path):\n#         zip_subdirectory(item_path, output_dir)\n\n# print(\"All subdirectories have been zipped.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:13:41.843024Z","iopub.execute_input":"2024-08-07T07:13:41.843285Z","iopub.status.idle":"2024-08-07T07:13:45.246586Z","shell.execute_reply.started":"2024-08-07T07:13:41.843263Z","shell.execute_reply":"2024-08-07T07:13:45.245598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wandb.init(project=\"maze-dqn\", name=\"stable-encode-maze-10-10-pengujian\")\n\n# from stable_baselines3 import DQN\n# env_test = CustomMazeEnvCnnML(maze_data_list=maze_data_list_alot, max_steps_per_episode=400, maze_size=10, random_start=True)\n\n# model = DQN.load(\"/kaggle/working/stable-encode-maze-10-10-testcount.zip\") \n# success_rate, avg_reward, avg_steps = evaluate_agent(model, env_test, num_episodes=1000)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T07:14:30.459353Z","iopub.execute_input":"2024-08-07T07:14:30.459963Z","iopub.status.idle":"2024-08-07T07:16:13.013656Z","shell.execute_reply.started":"2024-08-07T07:14:30.459932Z","shell.execute_reply":"2024-08-07T07:16:13.012631Z"},"trusted":true},"execution_count":null,"outputs":[]}]}